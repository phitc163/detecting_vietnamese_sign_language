# -*- coding: utf-8 -*-
"""Recognizing Vietnamese sign language.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12GplxIayzLvGe8BlvRCf11L0bmy-2eMW
"""

!pip install mediapipe

from google.colab import drive
drive.mount(r'/content/drive')

import cv2
import numpy as np
import os
from matplotlib import pyplot as plt
import time
import mediapipe as mp
import pandas as pd

from google.colab.patches import cv2_imshow

mp_holistic = mp.solutions.holistic # Holistic model
mp_drawing = mp.solutions.drawing_utils # Drawing utilities

def mediapipe_detection(image, model):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB
    image.flags.writeable = False                  # Image is no longer writeable
    results = model.process(image)                 # Make prediction
    image.flags.writeable = True                   # Image is now writeable
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR
    return image, results

def draw_styled_landmarks(image, results):
    # Draw pose connections
    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,
                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),
                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)
                             )

def extract_keypoints(results):
    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*3)
    return pose

lm_list = []

DATA_PATH = r'/content/drive/MyDrive/Sign language data/'

actions = np.array(['An', 'AnhHung', 'HomQua', 'DauVaiPhai', 'Cam', 'Chay', 'TuDo', 'CaiDat', 'BenPhai', 'TheDuc'])

with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
    # Loop through each action
    for action in actions:
        action_path = os.path.join(DATA_PATH, action)

        # Check if the action folder exists
        if os.path.exists(action_path):
            print(f"Processing videos for action: {action}")

            csv_folder = os.path.join(DATA_PATH, 'CSV')
            os.makedirs(csv_folder, exist_ok=True)

            for filename in os.listdir(action_path):
                video_path = os.path.join(action_path, filename)

                cap = cv2.VideoCapture(video_path)

                print(f"Processing video: {filename}")

                lm_list = []

                for frame_num in range(20):
                    ret, frame = cap.read()

                    if not ret:
                        break

                    # Make detections
                    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    image.flags.writeable = False
                    results = holistic.process(image)
                    image.flags.writeable = True
                    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

                    # Extract keypoints
                    keypoints = extract_keypoints(results)
                    lm_list.append(keypoints)

                    cv2.waitKey(10)

                df = pd.DataFrame(lm_list)

                video_path = DATA_PATH + action + "CSV/" + filename + ".csv"
                df.to_csv(video_path, index=False)

                df.to_csv(video_path, index=False)

                cap.release()
                cv2.destroyAllWindows()

from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

label_map = {label:num for num, label in enumerate(actions)}

label_map

sequences, labels = [], []
for action in actions:
    action_path = os.path.join(DATA_PATH, action)
    for filename in os.listdir(action_path):
        video_path = DATA_PATH + action + "CSV/" + filename + ".csv"
        df = pd.read_csv(video_path)
        sequences.append(df)
        labels.append(label_map[action])

np.array(sequences).shape

np.array(labels).shape

X = np.array(sequences)

y = to_categorical(labels).astype(int)

X_train, X_, y_train, y_ = train_test_split(X, y, test_size=0.3)
X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size=0.5)

y_test.shape

y_val.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import TensorBoard

log_dir = os.path.join('Logs')
tb_callback = TensorBoard(log_dir=log_dir)

model = Sequential()
model.add(LSTM(256, return_sequences=True, activation='relu', input_shape=(20,99)))
model.add(LSTM(128, return_sequences=True, activation='relu'))
model.add(LSTM(64, return_sequences=False, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(actions.shape[0], activation='softmax'))

model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])

model.summary()

model.fit(X_train, y_train, epochs=50, callbacks=[tb_callback], validation_data=(X_val, y_val), batch_size=32)

res = model.predict(X_test)

actions[np.argmax(res[27])]

actions[np.argmax(y_test[27])]

model.save('Sign_02.h5')

del model

model.load_weights('Sign_02.h5')

from sklearn.metrics import multilabel_confusion_matrix, accuracy_score

yhat = model.predict(X_test)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

ytrue = np.argmax(y_test, axis=1).tolist()
yhat = np.argmax(yhat, axis=1).tolist()

precision = precision_score(ytrue, yhat, average='weighted')
recall = recall_score(ytrue, yhat, average='weighted')
f1 = f1_score(ytrue, yhat, average='weighted')
accuracy = accuracy_score(ytrue, yhat)

print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')
print(f'Accuracy: {accuracy:.2f}')

cm = confusion_matrix(ytrue, yhat)

accuracy = accuracy_score(ytrue, yhat)

class_labels = ['An', 'AnhHung', 'HomQua', 'DauVaiPhai', 'Cam', 'Chay', 'TuDo', 'CaiDat', 'BenPhai', 'TheDuc']

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

multilabel_confusion_matrix(ytrue, yhat)